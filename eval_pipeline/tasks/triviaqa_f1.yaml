task: triviaqa_f1


# Same dataset as the built-in TriviaQA task
dataset_path: trivia_qa
dataset_name: unfiltered

training_split: null
validation_split: validation
test_split: null

output_type: generate_until

generation_kwargs:
  max_new_tokens: 32
  until:
    - "\n"

doc_to_text: |
  Question: {{ question }}
  Answer:
doc_to_target: "{{ answer.value if answer is mapping else answer }}"

# This is where we define which metrics we want.
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true
    

  - metric: f1
    aggregation: mean
    higher_is_better: true
    