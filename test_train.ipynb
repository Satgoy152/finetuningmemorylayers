{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "426d9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified imports for single GPU\n",
    "from logging import getLogger\n",
    "import math\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Import the embedding bag\n",
    "# from memory_layers.xformer_embeddingbag import xFormerEmbeddingBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d040a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagoyal/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced layer 6 FFN with memory layer\n",
      "Replaced layer 12 FFN with memory layer\n",
      "Replaced layer 18 FFN with memory layer\n",
      "‚úì Trainable: model.layers.6.mlp.keys\n",
      "‚úì Trainable: model.layers.6.mlp.values.weight\n",
      "‚úì Trainable: model.layers.6.mlp.value_proj.weight\n",
      "‚úì Trainable: model.layers.6.mlp.value_proj.bias\n",
      "‚úì Trainable: model.layers.6.mlp.swilu_projection.weight\n",
      "‚úì Trainable: model.layers.6.mlp.swilu_projection.bias\n",
      "‚úì Trainable: model.layers.6.mlp.query_proj.query_mlps.0.weight\n",
      "‚úì Trainable: model.layers.6.mlp.query_proj.query_mlps.0.bias\n",
      "‚úì Trainable: model.layers.12.mlp.keys\n",
      "‚úì Trainable: model.layers.12.mlp.values.weight\n",
      "‚úì Trainable: model.layers.12.mlp.value_proj.weight\n",
      "‚úì Trainable: model.layers.12.mlp.value_proj.bias\n",
      "‚úì Trainable: model.layers.12.mlp.swilu_projection.weight\n",
      "‚úì Trainable: model.layers.12.mlp.swilu_projection.bias\n",
      "‚úì Trainable: model.layers.12.mlp.query_proj.query_mlps.0.weight\n",
      "‚úì Trainable: model.layers.12.mlp.query_proj.query_mlps.0.bias\n",
      "‚úì Trainable: model.layers.18.mlp.keys\n",
      "‚úì Trainable: model.layers.18.mlp.values.weight\n",
      "‚úì Trainable: model.layers.18.mlp.value_proj.weight\n",
      "‚úì Trainable: model.layers.18.mlp.value_proj.bias\n",
      "‚úì Trainable: model.layers.18.mlp.swilu_projection.weight\n",
      "‚úì Trainable: model.layers.18.mlp.swilu_projection.bias\n",
      "‚úì Trainable: model.layers.18.mlp.query_proj.query_mlps.0.weight\n",
      "‚úì Trainable: model.layers.18.mlp.query_proj.query_mlps.0.bias\n",
      "\n",
      "Trainable: 52,011,264 / 506,820,736 (10.26%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from memory_layers.memory import HashingMemory, ProductKeyArgs\n",
    "device = \"cuda\"\n",
    "# Load Qwen0.5 Instruct\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", dtype=torch.float16).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", dtype=torch.float16)\n",
    "\n",
    "# Qwen0.5 specs: 896 hidden_dim, 24 layers\n",
    "hidden_dim = 896\n",
    "layers_to_replace = [6, 12, 18]  # Which FFN layers to replace\n",
    "\n",
    "# Replace FFNs with Memory Layers\n",
    "for layer_idx in layers_to_replace:\n",
    "    layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Create memory layer\n",
    "    memory_layer = HashingMemory(\n",
    "        input_dim=hidden_dim,\n",
    "        output_dim=hidden_dim,\n",
    "        mem_n_keys=128,          # Memory size = 512¬≤ = 262k entries\n",
    "        mem_heads=4,\n",
    "        mem_knn=16,\n",
    "        mem_k_dim=256,\n",
    "        mem_v_dim=-1,            # Auto: uses output_dim\n",
    "        swilu_projection=True,\n",
    "        value_fixed_lr=0.001,\n",
    "        mem_share_values=False,  # Don't share across layers for fine-tuning\n",
    "    )\n",
    "    \n",
    "    # Initialize the memory layer\n",
    "    memory_layer.reset_parameters()\n",
    "    memory_layer.to(device)\n",
    "    \n",
    "    # Replace the FFN (MLP) with memory layer\n",
    "    original_mlp = layer.mlp\n",
    "    layer.mlp = memory_layer\n",
    "    \n",
    "    print(f\"Replaced layer {layer_idx} FFN with memory layer\")\n",
    "\n",
    "# FREEZE EVERYTHING EXCEPT MEMORY LAYERS\n",
    "for name, param in model.named_parameters():\n",
    "    if 'mlp' in name and any(f'layers.{idx}.' in name for idx in layers_to_replace):\n",
    "        # This is a memory layer parameter - keep trainable\n",
    "        param.requires_grad = True\n",
    "        print(f\"‚úì Trainable: {name}\")\n",
    "    else:\n",
    "        # Freeze all other parameters\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Verify what's trainable\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTrainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c85e0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "import torch\n",
    "import wandb  # Optional but highly recommended\n",
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "class MemoryLayerMonitorAndCheckpoint(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Combined callback for:\n",
    "    1.  Monitoring memory layer training health\n",
    "    2. Safe checkpoint saving with safetensors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, layers_to_check=[6, 12, 18], \n",
    "                 save_every=500, keep_last=2, monitor_every=50):\n",
    "        # Monitoring\n",
    "        self.model = model\n",
    "        self.layers_to_check = layers_to_check\n",
    "        self.monitor_every = monitor_every\n",
    "        self.initial_params = {}\n",
    "        \n",
    "        # Checkpointing\n",
    "        self.save_every = save_every\n",
    "        self.keep_last = keep_last\n",
    "        self.checkpoints = []\n",
    "        \n",
    "        # Store initial parameter values for monitoring\n",
    "        for idx in layers_to_check:\n",
    "            layer = model.model.layers[idx].mlp\n",
    "            self.initial_params[f\"layer_{idx}_keys\"] = layer.keys.data.clone()\n",
    "            self.initial_params[f\"layer_{idx}_values\"] = layer.values.weight.data.clone()\n",
    "    \n",
    "    def on_step_end(self, args, state, control, model=None, tokenizer=None, **kwargs):\n",
    "        step = state.global_step\n",
    "        \n",
    "        # ================================================================\n",
    "        # MONITORING (every N steps)\n",
    "        # ================================================================\n",
    "        if step % self.monitor_every == 0 and step > 0:\n",
    "            self._monitor_health(step)\n",
    "        \n",
    "        # ================================================================\n",
    "        # CHECKPOINTING (every M steps)\n",
    "        # ================================================================\n",
    "        if step % self.save_every == 0 and step > 0:\n",
    "            self._save_checkpoint(step, state, model, tokenizer)\n",
    "    \n",
    "    def _monitor_health(self, step):\n",
    "        \"\"\"Monitor memory layer training health\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üîç MEMORY LAYER HEALTH CHECK - Step {step}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        all_healthy = True\n",
    "        \n",
    "        for idx in self.layers_to_check:\n",
    "            layer = self.model.model.layers[idx].mlp\n",
    "            \n",
    "            # Check parameter changes\n",
    "            keys_diff = (\n",
    "                layer.keys.data - self.initial_params[f\"layer_{idx}_keys\"]\n",
    "            ).abs().mean().item()\n",
    "            values_diff = (\n",
    "                layer.values.weight.data - self.initial_params[f\"layer_{idx}_values\"]\n",
    "            ).abs().mean().item()\n",
    "            \n",
    "            # Check gradients\n",
    "            keys_grad = layer.keys.grad.norm().item() if layer.keys.grad is not None else 0.0\n",
    "            values_grad = (\n",
    "                layer.values.weight.grad.norm().item() \n",
    "                if layer.values.weight.grad is not None else 0.0\n",
    "            )\n",
    "            \n",
    "            # Parameter statistics\n",
    "            keys_mean = layer.keys.data.mean().item()\n",
    "            keys_std = layer.keys.data.std().item()\n",
    "            values_mean = layer.values.weight.data.mean().item()\n",
    "            values_std = layer.values.weight.data.std().item()\n",
    "            \n",
    "            print(f\"\\nüìä Layer {idx} Memory:\")\n",
    "            print(f\"  Parameters:\")\n",
    "            print(f\"    Keys:   mean={keys_mean:+.4f}, std={keys_std:.4f}\")\n",
    "            print(f\"    Values: mean={values_mean:+.4f}, std={values_std:.4f}\")\n",
    "            print(f\"  Changes since start:\")\n",
    "            print(f\"    Keys:   {keys_diff:.6f} {'‚úÖ' if keys_diff > 1e-6 else '‚ùå FROZEN'}\")\n",
    "            print(f\"    Values: {values_diff:.6f} {'‚úÖ' if values_diff > 1e-6 else '‚ùå FROZEN'}\")\n",
    "            print(f\"  Gradient norms:\")\n",
    "            print(f\"    Keys:   {keys_grad:.4f} {'‚úÖ' if keys_grad > 0 else '‚ùå NO GRAD'}\")\n",
    "            print(f\"    Values: {values_grad:.4f} {'‚úÖ' if values_grad > 0 else '‚ùå NO GRAD'}\")\n",
    "            \n",
    "            # Health checks\n",
    "            if keys_diff < 1e-8 and step > 100:\n",
    "                print(f\"  ‚ö†Ô∏è  WARNING: Keys not updating!\")\n",
    "                all_healthy = False\n",
    "            if values_diff < 1e-8 and step > 100:\n",
    "                print(f\"  ‚ö†Ô∏è  WARNING: Values not updating!\")\n",
    "                all_healthy = False\n",
    "            if keys_grad == 0.0:\n",
    "                print(f\"  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\")\n",
    "                all_healthy = False\n",
    "            if values_grad == 0.0:\n",
    "                print(f\"  ‚ö†Ô∏è  WARNING: No gradient flow to values!\")\n",
    "                all_healthy = False\n",
    "        \n",
    "        if all_healthy:\n",
    "            print(f\"\\n‚úÖ All memory layers healthy!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Some memory layers need attention!\")\n",
    "        \n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def _save_checkpoint(self, step, state, model, tokenizer):\n",
    "        \"\"\"Save checkpoint safely with safetensors\"\"\"\n",
    "        checkpoint_dir = f\"./checkpoints/step-{step}\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nüíæ Saving checkpoint at step {step}...\")\n",
    "        \n",
    "        try:\n",
    "            # Save model with safetensors (no JSON serialization issues)\n",
    "            model.save_pretrained(\n",
    "                checkpoint_dir, \n",
    "                safe_serialization=True\n",
    "            )\n",
    "            \n",
    "            # Save tokenizer\n",
    "            if tokenizer:\n",
    "                tokenizer.save_pretrained(checkpoint_dir)\n",
    "            \n",
    "            # Save minimal training state (safe to serialize)\n",
    "            training_state = {\n",
    "                'step': step,\n",
    "                'epoch': state.epoch,\n",
    "                'global_step': state.global_step,\n",
    "            }\n",
    "            \n",
    "            # Add last loss if available\n",
    "            if state.log_history:\n",
    "                last_log = state.log_history[-1]\n",
    "                if 'loss' in last_log:\n",
    "                    training_state['loss'] = last_log['loss']\n",
    "            \n",
    "            torch.save(\n",
    "                training_state, \n",
    "                os.path.join(checkpoint_dir, 'training_state.pt')\n",
    "            )\n",
    "            \n",
    "            # Track checkpoints\n",
    "            self.checkpoints.append(checkpoint_dir)\n",
    "            \n",
    "            # Remove old checkpoints (keep only last N)\n",
    "            if len(self.checkpoints) > self.keep_last:\n",
    "                old_checkpoint = self.checkpoints.pop(0)\n",
    "                if os.path.exists(old_checkpoint):\n",
    "                    shutil.rmtree(old_checkpoint)\n",
    "                    print(f\"  üóëÔ∏è  Removed old checkpoint: {os.path.basename(old_checkpoint)}\")\n",
    "            \n",
    "            print(f\"  ‚úÖ Checkpoint saved: {checkpoint_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to save checkpoint: {e}\")\n",
    "            # Continue training even if checkpoint fails\n",
    "    \n",
    "    def on_train_end(self, args, state, control, model=None, tokenizer=None, **kwargs):\n",
    "        \"\"\"Save final model at end of training\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"üèÅ TRAINING COMPLETE - Saving final model\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        final_dir = \"./qwen_memory_final\"\n",
    "        os.makedirs(final_dir, exist_ok=True)\n",
    "        \n",
    "        model.save_pretrained(final_dir, safe_serialization=True)\n",
    "        if tokenizer:\n",
    "            tokenizer.save_pretrained(final_dir)\n",
    "        \n",
    "        # Save final statistics\n",
    "        final_stats = {\n",
    "            'total_steps': state.global_step,\n",
    "            'total_epochs': state.epoch,\n",
    "        }\n",
    "        \n",
    "        if state.log_history:\n",
    "            losses = [log['loss'] for log in state.log_history if 'loss' in log]\n",
    "            if losses:\n",
    "                final_stats['final_loss'] = losses[-1]\n",
    "                final_stats['initial_loss'] = losses[0]\n",
    "                final_stats['loss_improvement'] = losses[0] - losses[-1]\n",
    "        \n",
    "        torch.save(final_stats, os.path.join(final_dir, 'final_stats.pt'))\n",
    "        \n",
    "        print(f\"‚úÖ Final model saved to: {final_dir}\")\n",
    "        print(f\"   Total steps: {state.global_step}\")\n",
    "        print(f\"   Total epochs: {state.epoch:.2f}\")\n",
    "        if 'loss_improvement' in final_stats:\n",
    "            print(f\"   Loss improvement: {final_stats['loss_improvement']:.4f}\")\n",
    "        print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# Initialize callback\n",
    "memory_monitor = MemoryLayerMonitorAndCheckpoint(model=model,\n",
    "    layers_to_check=[6, 12, 18],    # Your memory layer indices\n",
    "    save_every=500,                  # Save checkpoint every 500 steps\n",
    "    keep_last=2,                     # Keep only 2 checkpoints\n",
    "    monitor_every=50,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313d50e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 7669\n",
      "Tokenized dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 7669\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load and filter OpenAssistant\n",
    "dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\")\n",
    "\n",
    "# Keep only high-quality English assistant responses\n",
    "filtered = dataset.filter(\n",
    "    lambda x: (\n",
    "        x['lang'] == 'en' and \n",
    "        x['role'] == 'assistant' and \n",
    "        x['rank'] == 0.0 and\n",
    "        len(x['text']) > 50  # Filter out very short responses\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Filtered dataset size: {len(filtered)}\")\n",
    "\n",
    "# Take subset\n",
    "dataset = filtered.select(range(min(20000, len(filtered))))\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    tokenize, \n",
    "    batched=True, \n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=4  # Speed up with multiprocessing\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset: {tokenized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6345a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Training arguments optimized for memory layers only\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen_memory_finetuned\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,  # Higher LR since only training memory\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,  # Log immediately\n",
    "    logging_dir=\"./logs\",\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=250,   \n",
    "    # Performance\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=False,  # Not needed with frozen base\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    \n",
    "    # Monitoring\n",
    "    report_to=\"tensorboard\",  # or \"wandb\" if you have it\n",
    "    # load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    save_strategy=\"no\",\n",
    "    \n",
    "    # Memory optimization\n",
    "    optim=\"adamw_torch_fused\",  # Faster optimizer\n",
    "    max_grad_norm=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e846d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting training...\n",
      "Total steps: 1437\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1440' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1440/1440 12:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.971200</td>\n",
       "      <td>1.873475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.718200</td>\n",
       "      <td>1.789377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.830100</td>\n",
       "      <td>1.702344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.619400</td>\n",
       "      <td>1.635654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.641900</td>\n",
       "      <td>1.603854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 50\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0001, std=0.0360\n",
      "    Values: mean=+0.0000, std=0.0334\n",
      "  Changes since start:\n",
      "    Keys:   0.000846 ‚úÖ\n",
      "    Values: 0.000810 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0361\n",
      "    Values: mean=-0.0000, std=0.0334\n",
      "  Changes since start:\n",
      "    Keys:   0.000830 ‚úÖ\n",
      "    Values: 0.000776 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0361\n",
      "    Values: mean=+0.0000, std=0.0334\n",
      "  Changes since start:\n",
      "    Keys:   0.000801 ‚úÖ\n",
      "    Values: 0.000749 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 100\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0360\n",
      "    Values: mean=+0.0000, std=0.0335\n",
      "  Changes since start:\n",
      "    Keys:   0.002264 ‚úÖ\n",
      "    Values: 0.001647 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0361\n",
      "    Values: mean=-0.0000, std=0.0335\n",
      "  Changes since start:\n",
      "    Keys:   0.002114 ‚úÖ\n",
      "    Values: 0.001463 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0363\n",
      "    Values: mean=+0.0000, std=0.0335\n",
      "  Changes since start:\n",
      "    Keys:   0.002260 ‚úÖ\n",
      "    Values: 0.001855 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 150\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0361\n",
      "    Values: mean=+0.0000, std=0.0335\n",
      "  Changes since start:\n",
      "    Keys:   0.003031 ‚úÖ\n",
      "    Values: 0.001971 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0362\n",
      "    Values: mean=-0.0000, std=0.0335\n",
      "  Changes since start:\n",
      "    Keys:   0.002900 ‚úÖ\n",
      "    Values: 0.001694 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0364\n",
      "    Values: mean=+0.0000, std=0.0336\n",
      "  Changes since start:\n",
      "    Keys:   0.003373 ‚úÖ\n",
      "    Values: 0.002592 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 200\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0362\n",
      "    Values: mean=+0.0000, std=0.0336\n",
      "  Changes since start:\n",
      "    Keys:   0.003620 ‚úÖ\n",
      "    Values: 0.002298 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0363\n",
      "    Values: mean=-0.0000, std=0.0335\n",
      "  Changes since start:\n",
      "    Keys:   0.003435 ‚úÖ\n",
      "    Values: 0.001832 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0366\n",
      "    Values: mean=+0.0000, std=0.0337\n",
      "  Changes since start:\n",
      "    Keys:   0.004146 ‚úÖ\n",
      "    Values: 0.003096 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 250\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0363\n",
      "    Values: mean=+0.0000, std=0.0336\n",
      "  Changes since start:\n",
      "    Keys:   0.004095 ‚úÖ\n",
      "    Values: 0.002546 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0364\n",
      "    Values: mean=-0.0000, std=0.0335\n",
      "  Changes since start:\n",
      "    Keys:   0.003861 ‚úÖ\n",
      "    Values: 0.001928 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0368\n",
      "    Values: mean=+0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.004792 ‚úÖ\n",
      "    Values: 0.003495 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 300\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0364\n",
      "    Values: mean=+0.0000, std=0.0336\n",
      "  Changes since start:\n",
      "    Keys:   0.004472 ‚úÖ\n",
      "    Values: 0.002753 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0365\n",
      "    Values: mean=-0.0000, std=0.0336\n",
      "  Changes since start:\n",
      "    Keys:   0.004240 ‚úÖ\n",
      "    Values: 0.002011 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0370\n",
      "    Values: mean=+0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.005343 ‚úÖ\n",
      "    Values: 0.003825 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 350\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0365\n",
      "    Values: mean=+0.0000, std=0.0337\n",
      "  Changes since start:\n",
      "    Keys:   0.004776 ‚úÖ\n",
      "    Values: 0.002930 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0365\n",
      "    Values: mean=-0.0000, std=0.0336\n",
      "  Changes since start:\n",
      "    Keys:   0.004558 ‚úÖ\n",
      "    Values: 0.002094 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0371\n",
      "    Values: mean=+0.0000, std=0.0339\n",
      "  Changes since start:\n",
      "    Keys:   0.005814 ‚úÖ\n",
      "    Values: 0.004075 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 400\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0366\n",
      "    Values: mean=+0.0000, std=0.0337\n",
      "  Changes since start:\n",
      "    Keys:   0.005063 ‚úÖ\n",
      "    Values: 0.003079 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0366\n",
      "    Values: mean=-0.0000, std=0.0336\n",
      "  Changes since start:\n",
      "    Keys:   0.004823 ‚úÖ\n",
      "    Values: 0.002179 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0373\n",
      "    Values: mean=+0.0000, std=0.0340\n",
      "  Changes since start:\n",
      "    Keys:   0.006214 ‚úÖ\n",
      "    Values: 0.004308 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 450\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0367\n",
      "    Values: mean=+0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.005329 ‚úÖ\n",
      "    Values: 0.003245 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0366\n",
      "    Values: mean=-0.0000, std=0.0336\n",
      "  Changes since start:\n",
      "    Keys:   0.005053 ‚úÖ\n",
      "    Values: 0.002251 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0374\n",
      "    Values: mean=+0.0000, std=0.0340\n",
      "  Changes since start:\n",
      "    Keys:   0.006585 ‚úÖ\n",
      "    Values: 0.004537 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 500\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0368\n",
      "    Values: mean=+0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.005585 ‚úÖ\n",
      "    Values: 0.003425 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0367\n",
      "    Values: mean=-0.0000, std=0.0336\n",
      "  Changes since start:\n",
      "    Keys:   0.005296 ‚úÖ\n",
      "    Values: 0.002318 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0376\n",
      "    Values: mean=+0.0000, std=0.0341\n",
      "  Changes since start:\n",
      "    Keys:   0.006950 ‚úÖ\n",
      "    Values: 0.004762 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üíæ Saving checkpoint at step 500...\n",
      "  ‚úÖ Checkpoint saved: ./checkpoints/step-500\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 550\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0371\n",
      "    Values: mean=+0.0000, std=0.0339\n",
      "  Changes since start:\n",
      "    Keys:   0.005879 ‚úÖ\n",
      "    Values: 0.003598 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0369\n",
      "    Values: mean=-0.0000, std=0.0337\n",
      "  Changes since start:\n",
      "    Keys:   0.005603 ‚úÖ\n",
      "    Values: 0.002378 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0381\n",
      "    Values: mean=+0.0000, std=0.0342\n",
      "  Changes since start:\n",
      "    Keys:   0.007392 ‚úÖ\n",
      "    Values: 0.004990 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 600\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0374\n",
      "    Values: mean=+0.0000, std=0.0339\n",
      "  Changes since start:\n",
      "    Keys:   0.006135 ‚úÖ\n",
      "    Values: 0.003774 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0371\n",
      "    Values: mean=-0.0000, std=0.0337\n",
      "  Changes since start:\n",
      "    Keys:   0.005806 ‚úÖ\n",
      "    Values: 0.002429 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0384\n",
      "    Values: mean=+0.0000, std=0.0343\n",
      "  Changes since start:\n",
      "    Keys:   0.007791 ‚úÖ\n",
      "    Values: 0.005219 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 650\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0376\n",
      "    Values: mean=+0.0000, std=0.0340\n",
      "  Changes since start:\n",
      "    Keys:   0.006397 ‚úÖ\n",
      "    Values: 0.003948 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0372\n",
      "    Values: mean=-0.0000, std=0.0337\n",
      "  Changes since start:\n",
      "    Keys:   0.006007 ‚úÖ\n",
      "    Values: 0.002487 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0388\n",
      "    Values: mean=+0.0000, std=0.0344\n",
      "  Changes since start:\n",
      "    Keys:   0.008140 ‚úÖ\n",
      "    Values: 0.005450 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 700\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0378\n",
      "    Values: mean=+0.0000, std=0.0340\n",
      "  Changes since start:\n",
      "    Keys:   0.006585 ‚úÖ\n",
      "    Values: 0.004111 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0374\n",
      "    Values: mean=-0.0000, std=0.0337\n",
      "  Changes since start:\n",
      "    Keys:   0.006191 ‚úÖ\n",
      "    Values: 0.002531 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0391\n",
      "    Values: mean=+0.0000, std=0.0345\n",
      "  Changes since start:\n",
      "    Keys:   0.008418 ‚úÖ\n",
      "    Values: 0.005653 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 750\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0379\n",
      "    Values: mean=+0.0000, std=0.0341\n",
      "  Changes since start:\n",
      "    Keys:   0.006751 ‚úÖ\n",
      "    Values: 0.004225 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0375\n",
      "    Values: mean=-0.0000, std=0.0337\n",
      "  Changes since start:\n",
      "    Keys:   0.006362 ‚úÖ\n",
      "    Values: 0.002574 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0393\n",
      "    Values: mean=+0.0000, std=0.0345\n",
      "  Changes since start:\n",
      "    Keys:   0.008672 ‚úÖ\n",
      "    Values: 0.005804 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 800\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0381\n",
      "    Values: mean=+0.0000, std=0.0341\n",
      "  Changes since start:\n",
      "    Keys:   0.006893 ‚úÖ\n",
      "    Values: 0.004330 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0376\n",
      "    Values: mean=-0.0000, std=0.0337\n",
      "  Changes since start:\n",
      "    Keys:   0.006513 ‚úÖ\n",
      "    Values: 0.002611 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0395\n",
      "    Values: mean=+0.0000, std=0.0346\n",
      "  Changes since start:\n",
      "    Keys:   0.008878 ‚úÖ\n",
      "    Values: 0.005933 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 850\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0382\n",
      "    Values: mean=+0.0000, std=0.0341\n",
      "  Changes since start:\n",
      "    Keys:   0.006984 ‚úÖ\n",
      "    Values: 0.004419 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0376\n",
      "    Values: mean=-0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.006614 ‚úÖ\n",
      "    Values: 0.002639 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0397\n",
      "    Values: mean=+0.0000, std=0.0346\n",
      "  Changes since start:\n",
      "    Keys:   0.009046 ‚úÖ\n",
      "    Values: 0.006046 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 900\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0383\n",
      "    Values: mean=+0.0000, std=0.0342\n",
      "  Changes since start:\n",
      "    Keys:   0.007067 ‚úÖ\n",
      "    Values: 0.004491 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0377\n",
      "    Values: mean=-0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.006690 ‚úÖ\n",
      "    Values: 0.002658 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0398\n",
      "    Values: mean=+0.0000, std=0.0347\n",
      "  Changes since start:\n",
      "    Keys:   0.009180 ‚úÖ\n",
      "    Values: 0.006133 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 950\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0384\n",
      "    Values: mean=+0.0000, std=0.0342\n",
      "  Changes since start:\n",
      "    Keys:   0.007138 ‚úÖ\n",
      "    Values: 0.004550 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0378\n",
      "    Values: mean=-0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.006752 ‚úÖ\n",
      "    Values: 0.002674 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0399\n",
      "    Values: mean=+0.0000, std=0.0347\n",
      "  Changes since start:\n",
      "    Keys:   0.009285 ‚úÖ\n",
      "    Values: 0.006209 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 1000\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0385\n",
      "    Values: mean=+0.0000, std=0.0342\n",
      "  Changes since start:\n",
      "    Keys:   0.007263 ‚úÖ\n",
      "    Values: 0.004606 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0378\n",
      "    Values: mean=-0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.006895 ‚úÖ\n",
      "    Values: 0.002692 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0401\n",
      "    Values: mean=+0.0000, std=0.0347\n",
      "  Changes since start:\n",
      "    Keys:   0.009426 ‚úÖ\n",
      "    Values: 0.006287 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üíæ Saving checkpoint at step 1000...\n",
      "  ‚úÖ Checkpoint saved: ./checkpoints/step-1000\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 1050\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0386\n",
      "    Values: mean=+0.0000, std=0.0342\n",
      "  Changes since start:\n",
      "    Keys:   0.007377 ‚úÖ\n",
      "    Values: 0.004656 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0379\n",
      "    Values: mean=-0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.007036 ‚úÖ\n",
      "    Values: 0.002708 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0403\n",
      "    Values: mean=+0.0000, std=0.0348\n",
      "  Changes since start:\n",
      "    Keys:   0.009594 ‚úÖ\n",
      "    Values: 0.006361 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 1100\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0387\n",
      "    Values: mean=+0.0000, std=0.0342\n",
      "  Changes since start:\n",
      "    Keys:   0.007460 ‚úÖ\n",
      "    Values: 0.004696 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0380\n",
      "    Values: mean=-0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.007128 ‚úÖ\n",
      "    Values: 0.002719 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0404\n",
      "    Values: mean=+0.0000, std=0.0348\n",
      "  Changes since start:\n",
      "    Keys:   0.009717 ‚úÖ\n",
      "    Values: 0.006414 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 1150\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0388\n",
      "    Values: mean=+0.0000, std=0.0343\n",
      "  Changes since start:\n",
      "    Keys:   0.007522 ‚úÖ\n",
      "    Values: 0.004724 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0380\n",
      "    Values: mean=-0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.007192 ‚úÖ\n",
      "    Values: 0.002726 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0405\n",
      "    Values: mean=+0.0000, std=0.0348\n",
      "  Changes since start:\n",
      "    Keys:   0.009804 ‚úÖ\n",
      "    Values: 0.006452 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 1200\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0388\n",
      "    Values: mean=+0.0000, std=0.0343\n",
      "  Changes since start:\n",
      "    Keys:   0.007560 ‚úÖ\n",
      "    Values: 0.004744 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0380\n",
      "    Values: mean=-0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.007238 ‚úÖ\n",
      "    Values: 0.002732 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0406\n",
      "    Values: mean=+0.0000, std=0.0348\n",
      "  Changes since start:\n",
      "    Keys:   0.009861 ‚úÖ\n",
      "    Values: 0.006479 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 1250\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0388\n",
      "    Values: mean=+0.0000, std=0.0343\n",
      "  Changes since start:\n",
      "    Keys:   0.007584 ‚úÖ\n",
      "    Values: 0.004757 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0381\n",
      "    Values: mean=-0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.007268 ‚úÖ\n",
      "    Values: 0.002736 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0407\n",
      "    Values: mean=+0.0000, std=0.0348\n",
      "  Changes since start:\n",
      "    Keys:   0.009897 ‚úÖ\n",
      "    Values: 0.006497 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 1300\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0389\n",
      "    Values: mean=+0.0000, std=0.0343\n",
      "  Changes since start:\n",
      "    Keys:   0.007599 ‚úÖ\n",
      "    Values: 0.004766 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0381\n",
      "    Values: mean=-0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.007285 ‚úÖ\n",
      "    Values: 0.002738 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0407\n",
      "    Values: mean=+0.0000, std=0.0348\n",
      "  Changes since start:\n",
      "    Keys:   0.009920 ‚úÖ\n",
      "    Values: 0.006508 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 1350\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0389\n",
      "    Values: mean=+0.0000, std=0.0343\n",
      "  Changes since start:\n",
      "    Keys:   0.007606 ‚úÖ\n",
      "    Values: 0.004769 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0381\n",
      "    Values: mean=-0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.007292 ‚úÖ\n",
      "    Values: 0.002738 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0407\n",
      "    Values: mean=+0.0000, std=0.0348\n",
      "  Changes since start:\n",
      "    Keys:   0.009930 ‚úÖ\n",
      "    Values: 0.006512 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üîç MEMORY LAYER HEALTH CHECK - Step 1400\n",
      "================================================================================\n",
      "\n",
      "üìä Layer 6 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0000, std=0.0389\n",
      "    Values: mean=+0.0000, std=0.0343\n",
      "  Changes since start:\n",
      "    Keys:   0.007608 ‚úÖ\n",
      "    Values: 0.004771 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 12 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=-0.0001, std=0.0381\n",
      "    Values: mean=-0.0000, std=0.0338\n",
      "  Changes since start:\n",
      "    Keys:   0.007294 ‚úÖ\n",
      "    Values: 0.002739 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "üìä Layer 18 Memory:\n",
      "  Parameters:\n",
      "    Keys:   mean=+0.0000, std=0.0407\n",
      "    Values: mean=+0.0000, std=0.0348\n",
      "  Changes since start:\n",
      "    Keys:   0.009934 ‚úÖ\n",
      "    Values: 0.006514 ‚úÖ\n",
      "  Gradient norms:\n",
      "    Keys:   0.0000 ‚ùå NO GRAD\n",
      "    Values: 0.0000 ‚ùå NO GRAD\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
      "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
      "\n",
      "‚ö†Ô∏è  Some memory layers need attention!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üèÅ TRAINING COMPLETE - Saving final model\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Final model saved to: ./qwen_memory_final\n",
      "   Total steps: 1440\n",
      "   Total epochs: 3.00\n",
      "   Loss improvement: 0.6245\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "‚úÖ Training complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Create trainer with callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    eval_dataset=tokenized.select(range(1000)),  # Use 1k for validation\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[memory_monitor],  # Add our custom monitor\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(f\"Total steps: {len(tokenized) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
    "\n",
    "# Train! \n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model loaded successfully!\n",
      "\n",
      "================================================================================\n",
      "Prompt: Explain quantum computing in simple terms:\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m response = \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtest_model\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_model\u001b[39m(prompt):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     inputs = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m.to(device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     36\u001b[39m         outputs = model.generate(\n\u001b[32m     37\u001b[39m             **inputs,\n\u001b[32m     38\u001b[39m             max_new_tokens=\u001b[32m100\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m             do_sample=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     42\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/transformers/tokenization_utils_base.py:3073\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3071\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   3072\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m3073\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3075\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/transformers/tokenization_utils_base.py:3183\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   3162\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   3163\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3180\u001b[39m         **kwargs,\n\u001b[32m   3181\u001b[39m     )\n\u001b[32m   3182\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3186\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3203\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/transformers/tokenization_utils_base.py:3258\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3229\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3230\u001b[39m \u001b[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[32m   3231\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3246\u001b[39m \u001b[33;03m        method).\u001b[39;00m\n\u001b[32m   3247\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3249\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3250\u001b[39m     padding=padding,\n\u001b[32m   3251\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3255\u001b[39m     **kwargs,\n\u001b[32m   3256\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3261\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3277\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3278\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3279\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/transformers/tokenization_utils_fast.py:627\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode_plus\u001b[39m(\n\u001b[32m    604\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    605\u001b[39m     text: Union[TextInput, PreTokenizedInput],\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m     **kwargs,\n\u001b[32m    625\u001b[39m ) -> BatchEncoding:\n\u001b[32m    626\u001b[39m     batched_input = [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     batched_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[32m    650\u001b[39m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[32m    651\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "\u001b[31mTypeError\u001b[39m: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from memory_layers.memory import HashingMemory\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Add memory layers\n",
    "for idx in [6, 12, 18]:\n",
    "    model.model.layers[idx].mlp = HashingMemory(\n",
    "        input_dim=896, output_dim=896, mem_n_keys=128, mem_heads=4,\n",
    "        mem_knn=16, mem_k_dim=256, mem_v_dim=-1, swilu_projection=True,\n",
    "        value_fixed_lr=0.001, mem_share_values=False\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "# Load weights (use safetensors if available, otherwise pytorch with weights_only=False)\n",
    "try:\n",
    "    state_dict = load_file(\"./qwen_memory_final/model.safetensors\")\n",
    "except:\n",
    "    state_dict = torch.load(\"./qwen_memory_final/pytorch_model.bin\", \n",
    "                           weights_only=False)  # ‚Üê Fix here\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", dtype=torch.float16)\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# Test generation\n",
    "def test_model(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device = \"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Try some prompts\n",
    "test_prompts = [\n",
    "    \"Explain quantum computing in simple terms:\",\n",
    "    \"Write a Python function to sort a list:\",\n",
    "    \"What are the health benefits of exercise?\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    response = test_model(prompt)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original Qwen model for comparison\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2. 5-0.5B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "base_model.to(\"cuda\")\n",
    "\n",
    "def compare_models(prompt):\n",
    "    # Your fine-tuned model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch. no_grad():\n",
    "        # Fine-tuned\n",
    "        ft_outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "        ft_response = tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Base\n",
    "        base_outputs = base_model.generate(**inputs, max_new_tokens=100)\n",
    "        base_response = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nüî∑ BASE MODEL:\")\n",
    "    print(base_response)\n",
    "    print(f\"\\nüî∂ FINE-TUNED (with memory layers):\")\n",
    "    print(ft_response)\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Test\n",
    "compare_models(\"Explain machine learning:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
