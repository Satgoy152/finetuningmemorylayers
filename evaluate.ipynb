{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from memory_layers import HashingMemory, ModelEvaluator\n",
    "from safetensors.torch import load_file\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "hidden_dim = 896\n",
    "layers_to_replace = [6, 12, 18]\n",
    "num_samples = 50  # Small sample for quick testing, increase for full eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b32a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluate Base Model\n",
    "print(\"Loading Base Model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float16).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, dtype=torch.float16)\n",
    "\n",
    "evaluator = ModelEvaluator(base_model, tokenizer, device=device)\n",
    "base_results = evaluator.evaluate_all(num_samples=num_samples)\n",
    "\n",
    "print(\"\\nBase Model Results:\")\n",
    "print(base_results)\n",
    "\n",
    "# Free up memory\n",
    "del base_model\n",
    "del evaluator\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d8af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Evaluate Fine-tuned Memory Model\n",
    "print(\"Loading Fine-tuned Memory Model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float16).to(device)\n",
    "\n",
    "# Add memory layers\n",
    "for idx in layers_to_replace:\n",
    "    mem_layer = HashingMemory(\n",
    "        input_dim=hidden_dim, output_dim=hidden_dim, mem_n_keys=128, mem_heads=4,\n",
    "        mem_knn=16, mem_k_dim=256, mem_v_dim=-1, swilu_projection=True,\n",
    "        value_fixed_lr=0.001, mem_share_values=False\n",
    "    )\n",
    "    model.model.layers[idx].mlp = mem_layer.to(device, dtype=model.dtype)\n",
    "\n",
    "# Load weights\n",
    "try:\n",
    "    state_dict = load_file(\"./qwen_memory_final/model.safetensors\")\n",
    "except:\n",
    "    state_dict = torch.load(\"./qwen_memory_final/pytorch_model.bin\", weights_only=False)\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "evaluator = ModelEvaluator(model, tokenizer, device=device)\n",
    "ft_results = evaluator.evaluate_all(num_samples=num_samples)\n",
    "\n",
    "print(\"\\nFine-tuned Model Results:\")\n",
    "print(ft_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compare Results\n",
    "df = pd.DataFrame([base_results, ft_results], index=['Base', 'Memory-Augmented'])\n",
    "print(\"\\nComparison Table:\")\n",
    "print(df)\n",
    "\n",
    "# Plot\n",
    "df.T.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
