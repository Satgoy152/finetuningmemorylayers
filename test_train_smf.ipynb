{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "426d9018",
      "metadata": {
        "id": "426d9018"
      },
      "outputs": [],
      "source": [
        "# Simplified imports for single GPU\n",
        "from logging import getLogger\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from memory_layers import HashingMemory, MemoryLayerMonitorAndCheckpoint, load_and_process_dataset, ModelEvaluator\n",
        "\n",
        "logger = getLogger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d040a3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d040a3d",
        "outputId": "4d5083ae-4b9a-41f4-c721-b0932a285a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Replaced layer 6 FFN with memory layer\n",
            "Replaced layer 12 FFN with memory layer\n",
            "Replaced layer 18 FFN with memory layer\n",
            "âœ“ Trainable: model.layers.6.mlp.keys\n",
            "âœ“ Trainable: model.layers.6.mlp.values.weight\n",
            "âœ“ Trainable: model.layers.6.mlp.value_proj.weight\n",
            "âœ“ Trainable: model.layers.6.mlp.value_proj.bias\n",
            "âœ“ Trainable: model.layers.6.mlp.swilu_projection.weight\n",
            "âœ“ Trainable: model.layers.6.mlp.swilu_projection.bias\n",
            "âœ“ Trainable: model.layers.6.mlp.query_proj.query_mlps.0.weight\n",
            "âœ“ Trainable: model.layers.6.mlp.query_proj.query_mlps.0.bias\n",
            "âœ“ Trainable: model.layers.12.mlp.keys\n",
            "âœ“ Trainable: model.layers.12.mlp.values.weight\n",
            "âœ“ Trainable: model.layers.12.mlp.value_proj.weight\n",
            "âœ“ Trainable: model.layers.12.mlp.value_proj.bias\n",
            "âœ“ Trainable: model.layers.12.mlp.swilu_projection.weight\n",
            "âœ“ Trainable: model.layers.12.mlp.swilu_projection.bias\n",
            "âœ“ Trainable: model.layers.12.mlp.query_proj.query_mlps.0.weight\n",
            "âœ“ Trainable: model.layers.12.mlp.query_proj.query_mlps.0.bias\n",
            "âœ“ Trainable: model.layers.18.mlp.keys\n",
            "âœ“ Trainable: model.layers.18.mlp.values.weight\n",
            "âœ“ Trainable: model.layers.18.mlp.value_proj.weight\n",
            "âœ“ Trainable: model.layers.18.mlp.value_proj.bias\n",
            "âœ“ Trainable: model.layers.18.mlp.swilu_projection.weight\n",
            "âœ“ Trainable: model.layers.18.mlp.swilu_projection.bias\n",
            "âœ“ Trainable: model.layers.18.mlp.query_proj.query_mlps.0.weight\n",
            "âœ“ Trainable: model.layers.18.mlp.query_proj.query_mlps.0.bias\n",
            "\n",
            "Trainable: 52,011,264 / 506,820,736 (10.26%)\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Load Qwen0.5 Instruct\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", dtype=torch.float16).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", dtype=torch.float16)\n",
        "\n",
        "# Qwen0.5 specs: 896 hidden_dim, 24 layers\n",
        "hidden_dim = 896\n",
        "layers_to_replace = [6, 12, 18]  # Which FFN layers to replace\n",
        "\n",
        "# Replace FFNs with Memory Layers\n",
        "for layer_idx in layers_to_replace:\n",
        "    layer = model.model.layers[layer_idx]\n",
        "    \n",
        "    # Create memory layer\n",
        "    memory_layer = HashingMemory(\n",
        "        input_dim=hidden_dim,\n",
        "        output_dim=hidden_dim,\n",
        "        mem_n_keys=128,          # Memory size = 512Â² = 262k entries\n",
        "        mem_heads=4,\n",
        "        mem_knn=16,\n",
        "        mem_k_dim=256,\n",
        "        mem_v_dim=-1,            # Auto: uses output_dim\n",
        "        swilu_projection=True,\n",
        "        value_fixed_lr=0.001,\n",
        "        mem_share_values=False,  # Don't share across layers for fine-tuning\n",
        "    )\n",
        "    \n",
        "    # Initialize the memory layer\n",
        "    memory_layer.reset_parameters()\n",
        "    # Ensure memory layer matches model dtype (float16)\n",
        "    memory_layer.to(device, dtype=model.dtype)\n",
        "    \n",
        "    # Replace the FFN (MLP) with memory layer\n",
        "    original_mlp = layer.mlp\n",
        "    layer.mlp = memory_layer\n",
        "    \n",
        "    print(f\"Replaced layer {layer_idx} FFN with memory layer\")\n",
        "\n",
        "# FREEZE EVERYTHING EXCEPT MEMORY LAYERS\n",
        "for name, param in model.named_parameters():\n",
        "    if 'mlp' in name and any(f'layers.{idx}.' in name for idx in layers_to_replace):\n",
        "        # This is a memory layer parameter - keep trainable\n",
        "        param.requires_grad = True\n",
        "        print(f\"âœ“ Trainable: {name}\")\n",
        "    else:\n",
        "        # Freeze all other parameters\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Verify what's trainable\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nTrainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tVbPFU5myZT4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVbPFU5myZT4",
        "outputId": "efabb4ee-ba43-4a11-fc2b-23e8c7c0ba87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… MemoryAccessLogger attached for layers: [6, 12, 18]\n"
          ]
        }
      ],
      "source": [
        "SPARSE_TOP_T = hidden_dim // 2  # top-t memory slots updated per layer, per batch\n",
        "BACKGROUND_MAX_BATCHES = 200 # how many background batches to use for DF\n",
        "\n",
        "class MemoryAccessLogger:\n",
        "    \"\"\"\n",
        "    Forward hook on each memory.values table that counts how often\n",
        "    each memory index is used in a batch.\n",
        "    \"\"\"\n",
        "    def __init__(self, values_module):\n",
        "        self.module = values_module\n",
        "        self.mem_size = values_module.weight.shape[0]\n",
        "        self.device = values_module.weight.device\n",
        "        self.batch_counts = torch.zeros(self.mem_size, dtype=torch.int64, device=self.device)\n",
        "        self.handle = self.module.register_forward_hook(self.hook)\n",
        "\n",
        "    def reset(self):\n",
        "        self.batch_counts.zero_()\n",
        "\n",
        "    def hook(self, module, inputs, output):\n",
        "        # inputs[0] is 'indices' passed to xFormerEmbeddingBag.forward\n",
        "        indices = inputs[0]  # shape: [N, bag_size]\n",
        "        with torch.no_grad():\n",
        "            flat = indices.reshape(-1).detach().to(\"cpu\")\n",
        "            counts_cpu = torch.bincount(flat, minlength=self.mem_size)\n",
        "            counts = counts_cpu.to(self.batch_counts.device)\n",
        "            self.batch_counts += counts\n",
        "\n",
        "    def close(self):\n",
        "        self.handle.remove()\n",
        "\n",
        "# Create one logger per memory layer\n",
        "mem_loggers = {}\n",
        "mem_sizes = {}\n",
        "\n",
        "for idx in layers_to_replace:  # e.g. [6, 12, 18]\n",
        "    mem_layer = model.model.layers[idx].mlp  # this is HashingMemory\n",
        "    mem_loggers[idx] = MemoryAccessLogger(mem_layer.values)\n",
        "    mem_sizes[idx] = mem_layer.values.weight.shape[0]\n",
        "\n",
        "print(\"\\nâœ… MemoryAccessLogger attached for layers:\", layers_to_replace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85e0f37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c85e0f37",
        "outputId": "d05b04ec-3d0e-48f4-8976-d1348d376097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered dataset size: 7669\n",
            "Tokenized dataset: Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 7669\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load and process dataset\n",
        "# First we will finetune on background data to populate memory\n",
        "\n",
        "tokenized = load_and_process_dataset(tokenizer, sample_size=20000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "313d50e2",
      "metadata": {
        "id": "313d50e2"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "# Training arguments optimized for memory layers only\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen_memory_finetuned\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=5e-4,  # Higher LR since only training memory\n",
        "    warmup_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,  # Log immediately\n",
        "    logging_dir=\"./logs\",\n",
        "    save_steps=500,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=250,\n",
        "    # Performance\n",
        "    fp16=False,\n",
        "    gradient_checkpointing=False,  # Not needed with frozen base\n",
        "    dataloader_num_workers=2,\n",
        "\n",
        "    # Monitoring\n",
        "    report_to=\"tensorboard\",  # or \"wandb\" if you have it\n",
        "    # load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    save_strategy=\"no\",\n",
        "\n",
        "    # Memory optimization\n",
        "    optim=\"adamw_torch_fused\",  # Faster optimizer\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ModelEvaluator(model, tokenizer, device=device)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "# Initialize callback\n",
        "memory_monitor = MemoryLayerMonitorAndCheckpoint(\n",
        "    model=model,\n",
        "    layers_to_check=layers_to_replace,\n",
        "    save_every=500,\n",
        "    keep_last=2,\n",
        "    monitor_every=50,\n",
        "    evaluator=evaluator,\n",
        "    eval_every=100,     # Run evaluation every 100 steps\n",
        "    eval_samples=20     # Small sample size for speed during training\n",
        ")\n",
        "\n",
        "# Create trainer with callback\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    eval_dataset=tokenized.select(range(1000)),  # Use 1k for validation\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[memory_monitor],  # Add our custom monitor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B3RHCohCytyC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3RHCohCytyC",
        "outputId": "1de14844-c612-40d2-eb2a-2f51ebd3942a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing background DF stats for sparse memory finetuning...\n",
            "Layer 6: background batches = 200\n",
            "Layer 12: background batches = 200\n",
            "Layer 18: background batches = 200\n",
            "\n",
            "Trainable parameters after adding sparse hooks:\n",
            "   model.layers.6.mlp.keys\n",
            "   model.layers.6.mlp.values.weight\n",
            "   model.layers.6.mlp.value_proj.weight\n",
            "   model.layers.6.mlp.value_proj.bias\n",
            "   model.layers.6.mlp.swilu_projection.weight\n",
            "   model.layers.6.mlp.swilu_projection.bias\n",
            "   model.layers.6.mlp.query_proj.query_mlps.0.weight\n",
            "   model.layers.6.mlp.query_proj.query_mlps.0.bias\n",
            "   model.layers.12.mlp.keys\n",
            "   model.layers.12.mlp.values.weight\n",
            "   model.layers.12.mlp.value_proj.weight\n",
            "   model.layers.12.mlp.value_proj.bias\n",
            "   model.layers.12.mlp.swilu_projection.weight\n",
            "   model.layers.12.mlp.swilu_projection.bias\n",
            "   model.layers.12.mlp.query_proj.query_mlps.0.weight\n",
            "   model.layers.12.mlp.query_proj.query_mlps.0.bias\n",
            "   model.layers.18.mlp.keys\n",
            "   model.layers.18.mlp.values.weight\n",
            "   model.layers.18.mlp.value_proj.weight\n",
            "   model.layers.18.mlp.value_proj.bias\n",
            "   model.layers.18.mlp.swilu_projection.weight\n",
            "   model.layers.18.mlp.swilu_projection.bias\n",
            "   model.layers.18.mlp.query_proj.query_mlps.0.weight\n",
            "   model.layers.18.mlp.query_proj.query_mlps.0.bias\n",
            "\n",
            "âœ… SparseMemoryTrainer is set up. Your existing `trainer.train()` cell will now do sparse memory finetuning.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import Trainer\n",
        "\n",
        "# 1) Compute background document frequency (DF) over memory slots\n",
        "background_dataset = tokenized\n",
        "\n",
        "bg_loader = DataLoader(\n",
        "    background_dataset,\n",
        "    batch_size=1, # small batch to avoid OOM\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "bg_df = {\n",
        "    idx: torch.zeros(mem_sizes[idx], dtype=torch.int64, device=device)\n",
        "    for idx in layers_to_replace\n",
        "}\n",
        "bg_num_batches = {idx: 0 for idx in layers_to_replace}\n",
        "\n",
        "model.eval()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Computing background DF stats for sparse memory finetuning...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for step, batch in enumerate(bg_loader):\n",
        "        if step >= BACKGROUND_MAX_BATCHES:\n",
        "            break\n",
        "\n",
        "        # Reset per-batch counts for each memory layer\n",
        "        for logger in mem_loggers.values():\n",
        "            logger.reset()\n",
        "\n",
        "        # Move to device and DROP labels to avoid computing loss\n",
        "        batch = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
        "\n",
        "        _ = model(**batch)  # forward only\n",
        "\n",
        "        # For each layer, mark which slots were touched in THIS batch\n",
        "        for idx, logger in mem_loggers.items():\n",
        "            used = (logger.batch_counts > 0).to(bg_df[idx].dtype)\n",
        "            bg_df[idx] += used\n",
        "            bg_num_batches[idx] += 1\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "for idx in layers_to_replace:\n",
        "    print(f\"Layer {idx}: background batches = {bg_num_batches[idx]}\")\n",
        "\n",
        "model.train()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 2) Per-slot trainability masks and gradient hooks:\n",
        "#    only the top-t (TF-IDF) slots will get gradient each batch.\n",
        "slot_train_masks = {\n",
        "    idx: torch.zeros(mem_sizes[idx], dtype=torch.bool, device=device)\n",
        "    for idx in layers_to_replace\n",
        "}\n",
        "\n",
        "def make_grad_hook(layer_idx):\n",
        "    def hook(grad):\n",
        "        \"\"\"\n",
        "        grad: [num_slots, value_dim] for this layer's values.weight\n",
        "        We zero-out rows whose mask is False, so only top-t slots get gradient.\n",
        "        \"\"\"\n",
        "        mask = slot_train_masks[layer_idx].to(grad.device)  # [num_slots]\n",
        "        return grad * mask.unsqueeze(-1)\n",
        "    return hook\n",
        "\n",
        "# Attach hooks to each memory values table\n",
        "for idx in layers_to_replace:\n",
        "    values_param = model.model.layers[idx].mlp.values.weight\n",
        "    values_param.register_hook(make_grad_hook(idx))\n",
        "\n",
        "print(\"\\nTrainable parameters after adding sparse hooks:\")\n",
        "for n, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(\"  \", n)\n",
        "\n",
        "# 3) SparseMemoryTrainer: override compute_loss with the new signature\n",
        "#    NOTE: num_items_in_batch is added to match HF Trainer API.\n",
        "\n",
        "class SparseMemoryTrainer(Trainer):\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        model,\n",
        "        inputs,\n",
        "        return_outputs: bool = False,\n",
        "        num_items_in_batch: int = None,\n",
        "    ):\n",
        "        # Reset per-batch counts for loggers\n",
        "        for logger in mem_loggers.values():\n",
        "            logger.reset()\n",
        "\n",
        "        # Standard forward pass (with labels, so we get loss)\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Compute TF-IDF score for each memory slot, per layer, and choose top-t\n",
        "        with torch.no_grad():\n",
        "            for idx in layers_to_replace:\n",
        "                logger = mem_loggers[idx]\n",
        "                counts = logger.batch_counts # c(i) on this batch\n",
        "                total = counts.sum()\n",
        "\n",
        "                if total == 0:\n",
        "                    slot_train_masks[idx].fill_(False)\n",
        "                    continue\n",
        "\n",
        "                # Term frequency: c(i) / sum_j c(j)\n",
        "                tf = counts.float() / total\n",
        "\n",
        "                # Document frequency from background: df(i) = #background batches where slot i was used\n",
        "                df = bg_df[idx].float()\n",
        "                N = float(bg_num_batches[idx])  # |B|\n",
        "\n",
        "                # Paperâ€™s TF-IDF:\n",
        "                # c(i)/sum_j c(j) * log( (|B| + 1) / (df(i) + 1) )\n",
        "                idf = torch.log((N + 1.0) / (df + 1.0))\n",
        "                tfidf = tf * idf.to(tf.device)\n",
        "\n",
        "                # Don't pick slots that weren't used in this batch\n",
        "                tfidf = tfidf.masked_fill(counts == 0, float(\"-inf\"))\n",
        "\n",
        "                num_active = int((counts > 0).sum().item())\n",
        "                k = min(SPARSE_TOP_T, num_active) if num_active > 0 else 0\n",
        "\n",
        "                if k == 0:\n",
        "                    slot_train_masks[idx].fill_(False)\n",
        "                    continue\n",
        "\n",
        "                # Top-t indices by TF-IDF\n",
        "                _, topk_idx = torch.topk(tfidf, k=k)\n",
        "                mask = torch.zeros_like(slot_train_masks[idx])\n",
        "                mask[topk_idx] = True\n",
        "                slot_train_masks[idx] = mask\n",
        "\n",
        "        if return_outputs:\n",
        "            return loss, outputs\n",
        "        return loss\n",
        "\n",
        "# Initialize Evaluator\n",
        "evaluator = ModelEvaluator(model, tokenizer, device=device)\n",
        "\n",
        "# Initialize callback\n",
        "memory_monitor = MemoryLayerMonitorAndCheckpoint(\n",
        "    model=model,\n",
        "    layers_to_check=layers_to_replace,\n",
        "    save_every=500,\n",
        "    keep_last=2,\n",
        "    monitor_every=50,\n",
        "    evaluator=evaluator,\n",
        "    eval_every=100,     # Run evaluation every 100 steps\n",
        "    eval_samples=20     # Small sample size for speed during training\n",
        ")\n",
        "\n",
        "# 4) Recreate 'trainer' using SparseMemoryTrainer (overwrites the earlier Trainer)\n",
        "trainer = SparseMemoryTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    eval_dataset=tokenized.select(range(1000)), # same eval subset as before\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[memory_monitor], # keep your monitor/checkpoints\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… SparseMemoryTrainer is set up. trainer.train() cell will now do sparse memory finetuning.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "6345a535",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "6345a535",
        "outputId": "468a6cdd-5d86-4d3a-e3a1-64f30438c6e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸš€ Starting training...\n",
            "Total steps: 1437\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   4/1440 00:03 < 45:01, 0.53 it/s, Epoch 0.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 536.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 152.12 MiB is free. Process 8929 has 14.59 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 526.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-279436352.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nâœ… Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-180814164.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Standard forward pass (with labels, so we get loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     decorate_autocast.__script_unsupported = (  # type: ignore[attr-defined]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         return CausalLMOutputWithPast(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mForCausalLMLoss\u001b[0;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Enable model parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mshift_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mfixed_cross_entropy\u001b[0;34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> torch.Tensor:\n\u001b[1;32m     35\u001b[0m     \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sum\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# just in case users pass an int for num_items_in_batch, which could be the case for custom trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3459\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 536.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 152.12 MiB is free. Process 8929 has 14.59 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 526.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "print(\"\\nðŸš€ Starting training...\")\n",
        "print(f\"Total steps: {len(tokenized) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nâœ… Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fbbab63",
      "metadata": {
        "id": "6fbbab63"
      },
      "outputs": [],
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "hidden_dim = 896\n",
        "layers_to_replace = [6, 12, 18]\n",
        "\n",
        "# Reload model for testing\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    dtype=torch.float16,\n",
        ").to(device)\n",
        "\n",
        "# Add memory layers\n",
        "for idx in layers_to_replace:\n",
        "    # Initialize and cast to correct device/dtype\n",
        "    mem_layer = HashingMemory(\n",
        "        input_dim=hidden_dim, output_dim=hidden_dim, mem_n_keys=128, mem_heads=4,\n",
        "        mem_knn=16, mem_k_dim=256, mem_v_dim=-1, swilu_projection=True,\n",
        "        value_fixed_lr=0.001, mem_share_values=False\n",
        "    )\n",
        "    # Important: Cast to model's dtype (float16) to avoid \"Half and Float\" errors\n",
        "    model.model.layers[idx].mlp = mem_layer.to(device, dtype=model.dtype)\n",
        "\n",
        "# Load weights\n",
        "try:\n",
        "    state_dict = load_file(\"./qwen_memory_final/model.safetensors\")\n",
        "except:\n",
        "    state_dict = torch.load(\"./qwen_memory_final/pytorch_model.bin\",\n",
        "                           weights_only=False)\n",
        "\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "print(\"\\nâœ… Model loaded successfully!\")\n",
        "\n",
        "# Test generation\n",
        "def test_model(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Try some prompts\n",
        "test_prompts = [\n",
        "    \"Explain quantum computing in simple terms:\",\n",
        "    \"Write a Python function to sort a list:\",\n",
        "    \"What are the health benefits of exercise?\",\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    response = test_model(prompt)\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9e1807",
      "metadata": {
        "id": "3f9e1807"
      },
      "outputs": [],
      "source": [
        "# Load original Qwen model for comparison\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "base_model.to(device)\n",
        "\n",
        "def compare_models(prompt):\n",
        "    # Your fine-tuned model\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Fine-tuned\n",
        "        ft_outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "        ft_response = tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Base\n",
        "        base_outputs = base_model.generate(**inputs, max_new_tokens=100)\n",
        "        base_response = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nðŸ”· BASE MODEL:\")\n",
        "    print(base_response)\n",
        "    print(f\"\\nðŸ”¶ FINE-TUNED (with memory layers):\")\n",
        "    print(ft_response)\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Test\n",
        "compare_models(\"Explain machine learning:\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
