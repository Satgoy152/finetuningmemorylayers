{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "426d9018",
      "metadata": {
        "id": "426d9018"
      },
      "outputs": [],
      "source": [
        "# Simplified imports for single GPU\n",
        "from logging import getLogger\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from memory_layers import HashingMemory, MemoryLayerMonitorAndCheckpoint, load_and_process_dataset, ModelEvaluator\n",
        "\n",
        "logger = getLogger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5d040a3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d040a3d",
        "outputId": "4d5083ae-4b9a-41f4-c721-b0932a285a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Replaced layer 6 FFN with memory layer\n",
            "Replaced layer 12 FFN with memory layer\n",
            "Replaced layer 18 FFN with memory layer\n",
            "‚úì Trainable: model.layers.6.mlp.keys\n",
            "‚úì Trainable: model.layers.6.mlp.values.weight\n",
            "‚úì Trainable: model.layers.6.mlp.value_proj.weight\n",
            "‚úì Trainable: model.layers.6.mlp.value_proj.bias\n",
            "‚úì Trainable: model.layers.6.mlp.swilu_projection.weight\n",
            "‚úì Trainable: model.layers.6.mlp.swilu_projection.bias\n",
            "‚úì Trainable: model.layers.6.mlp.query_proj.query_mlps.0.weight\n",
            "‚úì Trainable: model.layers.6.mlp.query_proj.query_mlps.0.bias\n",
            "‚úì Trainable: model.layers.12.mlp.keys\n",
            "‚úì Trainable: model.layers.12.mlp.values.weight\n",
            "‚úì Trainable: model.layers.12.mlp.value_proj.weight\n",
            "‚úì Trainable: model.layers.12.mlp.value_proj.bias\n",
            "‚úì Trainable: model.layers.12.mlp.swilu_projection.weight\n",
            "‚úì Trainable: model.layers.12.mlp.swilu_projection.bias\n",
            "‚úì Trainable: model.layers.12.mlp.query_proj.query_mlps.0.weight\n",
            "‚úì Trainable: model.layers.12.mlp.query_proj.query_mlps.0.bias\n",
            "‚úì Trainable: model.layers.18.mlp.keys\n",
            "‚úì Trainable: model.layers.18.mlp.values.weight\n",
            "‚úì Trainable: model.layers.18.mlp.value_proj.weight\n",
            "‚úì Trainable: model.layers.18.mlp.value_proj.bias\n",
            "‚úì Trainable: model.layers.18.mlp.swilu_projection.weight\n",
            "‚úì Trainable: model.layers.18.mlp.swilu_projection.bias\n",
            "‚úì Trainable: model.layers.18.mlp.query_proj.query_mlps.0.weight\n",
            "‚úì Trainable: model.layers.18.mlp.query_proj.query_mlps.0.bias\n",
            "\n",
            "Trainable: 52,011,264 / 506,820,736 (10.26%)\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Load Qwen0.5 Instruct\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", dtype=torch.float16).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", dtype=torch.float16)\n",
        "\n",
        "# Qwen0.5 specs: 896 hidden_dim, 24 layers\n",
        "hidden_dim = 896\n",
        "layers_to_replace = [6, 12, 18]  # Which FFN layers to replace\n",
        "\n",
        "# Replace FFNs with Memory Layers\n",
        "for layer_idx in layers_to_replace:\n",
        "    layer = model.model.layers[layer_idx]\n",
        "    \n",
        "    # Create memory layer\n",
        "    memory_layer = HashingMemory(\n",
        "        input_dim=hidden_dim,\n",
        "        output_dim=hidden_dim,\n",
        "        mem_n_keys=128,          # Memory size = 512¬≤ = 262k entries\n",
        "        mem_heads=4,\n",
        "        mem_knn=16,\n",
        "        mem_k_dim=256,\n",
        "        mem_v_dim=-1,            # Auto: uses output_dim\n",
        "        swilu_projection=True,\n",
        "        value_fixed_lr=0.001,\n",
        "        mem_share_values=False,  # Don't share across layers for fine-tuning\n",
        "    )\n",
        "    \n",
        "    # Initialize the memory layer\n",
        "    memory_layer.reset_parameters()\n",
        "    # Ensure memory layer matches model dtype (float16)\n",
        "    memory_layer.to(device, dtype=model.dtype)\n",
        "    \n",
        "    # Replace the FFN (MLP) with memory layer\n",
        "    original_mlp = layer.mlp\n",
        "    layer.mlp = memory_layer\n",
        "    \n",
        "    print(f\"Replaced layer {layer_idx} FFN with memory layer\")\n",
        "\n",
        "# FREEZE EVERYTHING EXCEPT MEMORY LAYERS\n",
        "for name, param in model.named_parameters():\n",
        "    if 'mlp' in name and any(f'layers.{idx}.' in name for idx in layers_to_replace):\n",
        "        # This is a memory layer parameter - keep trainable\n",
        "        param.requires_grad = True\n",
        "        print(f\"‚úì Trainable: {name}\")\n",
        "    else:\n",
        "        # Freeze all other parameters\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Verify what's trainable\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nTrainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c85e0f37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c85e0f37",
        "outputId": "d05b04ec-3d0e-48f4-8976-d1348d376097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered dataset size: 22596\n",
            "Tokenized dataset: Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 20000\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load and process dataset\n",
        "# First we will finetune on background data to populate memory\n",
        "\n",
        "tokenized = load_and_process_dataset(tokenizer, sample_size=20000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "313d50e2",
      "metadata": {
        "id": "313d50e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "# Training arguments optimized for memory layers only\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen_memory_finetuned\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=5e-4,  # Higher LR since only training memory\n",
        "    warmup_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,  # Log immediately\n",
        "    logging_dir=\"./logs\",\n",
        "    save_steps=500,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=250,\n",
        "    # Performance\n",
        "    fp16=False,\n",
        "    gradient_checkpointing=False,  # Not needed with frozen base\n",
        "    dataloader_num_workers=2,\n",
        "\n",
        "    # Monitoring\n",
        "    report_to=\"tensorboard\",  # or \"wandb\" if you have it\n",
        "    # load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    save_strategy=\"no\",\n",
        "\n",
        "    # Memory optimization\n",
        "    optim=\"adamw_torch_fused\",  # Faster optimizer\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ModelEvaluator(model, tokenizer, device=device)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "# Initialize callback\n",
        "memory_monitor = MemoryLayerMonitorAndCheckpoint(\n",
        "    model=model,\n",
        "    layers_to_check=layers_to_replace,\n",
        "    save_every=500,\n",
        "    keep_last=2,\n",
        "    monitor_every=50,\n",
        "    evaluator=evaluator,\n",
        "    eval_every=100,     # Run evaluation every 100 steps\n",
        "    eval_samples=50     # Small sample size for speed during training\n",
        ")\n",
        "\n",
        "# Create trainer with callback\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    eval_dataset=tokenized.select(range(1000)),  # Use 1k for validation\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[memory_monitor],  # Add our custom monitor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "86bc7fb3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Starting training...\n",
            "Total steps: 2500\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 100/2500 00:40 < 16:43, 2.39 it/s, Epoch 0.08/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üîç MEMORY LAYER HEALTH CHECK - Step 50\n",
            "================================================================================\n",
            "\n",
            "üìä Layer 6 Memory:\n",
            "  Parameters:\n",
            "    Keys:   mean=+nan, std=nan\n",
            "    Values: mean=+nan, std=nan\n",
            "  Changes since start:\n",
            "    Keys:   nan ‚ùå FROZEN\n",
            "    Values: nan ‚ùå FROZEN\n",
            "  Gradient norms:\n",
            "    Keys:   0.0000 ‚ùå NO GRAD\n",
            "    Values: 0.0000 ‚ùå NO GRAD\n",
            "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
            "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
            "\n",
            "üìä Layer 12 Memory:\n",
            "  Parameters:\n",
            "    Keys:   mean=+nan, std=nan\n",
            "    Values: mean=+nan, std=nan\n",
            "  Changes since start:\n",
            "    Keys:   nan ‚ùå FROZEN\n",
            "    Values: nan ‚ùå FROZEN\n",
            "  Gradient norms:\n",
            "    Keys:   0.0000 ‚ùå NO GRAD\n",
            "    Values: 0.0000 ‚ùå NO GRAD\n",
            "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
            "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
            "\n",
            "üìä Layer 18 Memory:\n",
            "  Parameters:\n",
            "    Keys:   mean=+nan, std=nan\n",
            "    Values: mean=+nan, std=nan\n",
            "  Changes since start:\n",
            "    Keys:   nan ‚ùå FROZEN\n",
            "    Values: nan ‚ùå FROZEN\n",
            "  Gradient norms:\n",
            "    Keys:   0.0000 ‚ùå NO GRAD\n",
            "    Values: 0.0000 ‚ùå NO GRAD\n",
            "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
            "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
            "\n",
            "‚ö†Ô∏è  Some memory layers need attention!\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'trivia_qa' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üîç MEMORY LAYER HEALTH CHECK - Step 100\n",
            "================================================================================\n",
            "\n",
            "üìä Layer 6 Memory:\n",
            "  Parameters:\n",
            "    Keys:   mean=+nan, std=nan\n",
            "    Values: mean=+nan, std=nan\n",
            "  Changes since start:\n",
            "    Keys:   nan ‚ùå FROZEN\n",
            "    Values: nan ‚ùå FROZEN\n",
            "  Gradient norms:\n",
            "    Keys:   0.0000 ‚ùå NO GRAD\n",
            "    Values: 0.0000 ‚ùå NO GRAD\n",
            "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
            "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
            "\n",
            "üìä Layer 12 Memory:\n",
            "  Parameters:\n",
            "    Keys:   mean=+nan, std=nan\n",
            "    Values: mean=+nan, std=nan\n",
            "  Changes since start:\n",
            "    Keys:   nan ‚ùå FROZEN\n",
            "    Values: nan ‚ùå FROZEN\n",
            "  Gradient norms:\n",
            "    Keys:   0.0000 ‚ùå NO GRAD\n",
            "    Values: 0.0000 ‚ùå NO GRAD\n",
            "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
            "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
            "\n",
            "üìä Layer 18 Memory:\n",
            "  Parameters:\n",
            "    Keys:   mean=+nan, std=nan\n",
            "    Values: mean=+nan, std=nan\n",
            "  Changes since start:\n",
            "    Keys:   nan ‚ùå FROZEN\n",
            "    Values: nan ‚ùå FROZEN\n",
            "  Gradient norms:\n",
            "    Keys:   0.0000 ‚ùå NO GRAD\n",
            "    Values: 0.0000 ‚ùå NO GRAD\n",
            "  ‚ö†Ô∏è  WARNING: No gradient flow to keys!\n",
            "  ‚ö†Ô∏è  WARNING: No gradient flow to values!\n",
            "\n",
            "‚ö†Ô∏è  Some memory layers need attention!\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "üìä RUNNING BENCHMARK EVALUATION - Step 100\n",
            "================================================================================\n",
            "Evaluating on TriviaQA (validation, 50 samples)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 138384/138384 [00:01<00:00, 136853.56 examples/s]\n",
            "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17944/17944 [00:00<00:00, 139900.05 examples/s]\n",
            "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17210/17210 [00:00<00:00, 420358.56 examples/s]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:41<00:00,  1.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TriviaQA Accuracy: 0.00%\n",
            "Evaluating on GSM8K (test, 50 samples)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7473/7473 [00:00<00:00, 189244.71 examples/s]\n",
            "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1319/1319 [00:00<00:00, 131627.10 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tokenized)\u001b[38;5;250m \u001b[39m//\u001b[38;5;250m \u001b[39m(training_args.per_device_train_batch_size\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39mtraining_args.gradient_accumulation_steps)\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39mtraining_args.num_train_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Train! \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Training complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/transformers/trainer.py:2755\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2753\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.global_step += \u001b[32m1\u001b[39m\n\u001b[32m   2754\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m) / steps_in_epoch\n\u001b[32m-> \u001b[39m\u001b[32m2755\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallback_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_step_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2756\u001b[39m     \u001b[38;5;28mself\u001b[39m._maybe_log_save_evaluate(\n\u001b[32m   2757\u001b[39m         tr_loss,\n\u001b[32m   2758\u001b[39m         grad_norm,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2764\u001b[39m         learning_rate=learning_rate,\n\u001b[32m   2765\u001b[39m     )\n\u001b[32m   2766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/transformers/trainer_callback.py:534\u001b[39m, in \u001b[36mCallbackHandler.on_step_end\u001b[39m\u001b[34m(self, args, state, control)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_step_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_step_end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/transformers/trainer_callback.py:556\u001b[39m, in \u001b[36mCallbackHandler.call_event\u001b[39m\u001b[34m(self, event, args, state, control, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, **kwargs):\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m         result = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[32m    569\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/memory_layers/callbacks.py:56\u001b[39m, in \u001b[36mMemoryLayerMonitorAndCheckpoint.on_step_end\u001b[39m\u001b[34m(self, args, state, control, model, tokenizer, **kwargs)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# ================================================================\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# EVALUATION (every K steps)\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# ================================================================\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluator \u001b[38;5;129;01mand\u001b[39;00m step % \u001b[38;5;28mself\u001b[39m.eval_every == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# ================================================================\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# CHECKPOINTING (every M steps)\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# ================================================================\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % \u001b[38;5;28mself\u001b[39m.save_every == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step > \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/memory_layers/callbacks.py:75\u001b[39m, in \u001b[36mMemoryLayerMonitorAndCheckpoint._run_evaluation\u001b[39m\u001b[34m(self, step, state)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# Log to file\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m state.is_world_process_zero:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/memory_layers/evaluation.py:134\u001b[39m, in \u001b[36mModelEvaluator.evaluate_all\u001b[39m\u001b[34m(self, num_samples)\u001b[39m\n\u001b[32m    132\u001b[39m results = {}\n\u001b[32m    133\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mtrivia_qa\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.evaluate_triviaqa(num_samples=num_samples)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mgsm8k\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_gsm8k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mhellaswag\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.evaluate_hellaswag(num_samples=num_samples)\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Add others as needed\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/memory_layers/evaluation.py:61\u001b[39m, in \u001b[36mModelEvaluator.evaluate_gsm8k\u001b[39m\u001b[34m(self, num_samples, split)\u001b[39m\n\u001b[32m     59\u001b[39m dataset = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mgsm8k\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m, split=split)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_samples:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m     64\u001b[39m total = \u001b[32m0\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/datasets/arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/datasets/fingerprint.py:442\u001b[39m, in \u001b[36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/datasets/arrow_dataset.py:4104\u001b[39m, in \u001b[36mDataset.select\u001b[39m\u001b[34m(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001b[39m\n\u001b[32m   4102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_range_contiguous(indices) \u001b[38;5;129;01mand\u001b[39;00m indices.start >= \u001b[32m0\u001b[39m:\n\u001b[32m   4103\u001b[39m         start, length = indices.start, indices.stop - indices.start\n\u001b[32m-> \u001b[39m\u001b[32m4104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select_contiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/datasets/arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/datasets/fingerprint.py:442\u001b[39m, in \u001b[36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/datasets/arrow_dataset.py:4168\u001b[39m, in \u001b[36mDataset._select_contiguous\u001b[39m\u001b[34m(self, start, length, new_fingerprint)\u001b[39m\n\u001b[32m   4165\u001b[39m _check_valid_indices_value(start + length - \u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[32m   4166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m length == \u001b[32m0\u001b[39m:\n\u001b[32m   4167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(\n\u001b[32m-> \u001b[39m\u001b[32m4168\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   4169\u001b[39m         info=\u001b[38;5;28mself\u001b[39m.info.copy(),\n\u001b[32m   4170\u001b[39m         split=\u001b[38;5;28mself\u001b[39m.split,\n\u001b[32m   4171\u001b[39m         fingerprint=new_fingerprint,\n\u001b[32m   4172\u001b[39m     )\n\u001b[32m   4173\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(\n\u001b[32m   4175\u001b[39m         \u001b[38;5;28mself\u001b[39m.data,\n\u001b[32m   4176\u001b[39m         info=\u001b[38;5;28mself\u001b[39m.info.copy(),\n\u001b[32m   (...)\u001b[39m\u001b[32m   4179\u001b[39m         fingerprint=new_fingerprint,\n\u001b[32m   4180\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/datasets/table.py:1065\u001b[39m, in \u001b[36mMemoryMappedTable.slice\u001b[39m\u001b[34m(self, offset, length)\u001b[39m\n\u001b[32m   1063\u001b[39m replays = \u001b[38;5;28mself\u001b[39m._append_replay(replay)\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# Use fast slicing here\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m MemoryMappedTable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfast_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.path, replays)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/datasets/table.py:150\u001b[39m, in \u001b[36mIndexedTableMixin.fast_slice\u001b[39m\u001b[34m(self, offset, length)\u001b[39m\n\u001b[32m    148\u001b[39m     batches[-\u001b[32m1\u001b[39m] = batches[-\u001b[32m1\u001b[39m].slice(\u001b[32m0\u001b[39m, offset + length - \u001b[38;5;28mself\u001b[39m._offsets[j])\n\u001b[32m    149\u001b[39m     batches[\u001b[32m0\u001b[39m] = batches[\u001b[32m0\u001b[39m].slice(offset - \u001b[38;5;28mself\u001b[39m._offsets[i])\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "print(\"\\nüöÄ Starting training...\")\n",
        "print(f\"Total steps: {len(tokenized) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
        "\n",
        "# Train! \n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "tVbPFU5myZT4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVbPFU5myZT4",
        "outputId": "efabb4ee-ba43-4a11-fc2b-23e8c7c0ba87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ MemoryAccessLogger attached for layers: [6, 12, 18]\n"
          ]
        }
      ],
      "source": [
        "SPARSE_TOP_T = hidden_dim // 2  # top-t memory slots updated per layer, per batch\n",
        "BACKGROUND_MAX_BATCHES = 200 # how many background batches to use for DF\n",
        "\n",
        "class MemoryAccessLogger:\n",
        "    \"\"\"\n",
        "    Forward hook on each memory.values table that counts how often\n",
        "    each memory index is used in a batch.\n",
        "    \"\"\"\n",
        "    def __init__(self, values_module):\n",
        "        self.module = values_module\n",
        "        self.mem_size = values_module.weight.shape[0]\n",
        "        self.device = values_module.weight.device\n",
        "        self.batch_counts = torch.zeros(self.mem_size, dtype=torch.int64, device=self.device)\n",
        "        self.handle = self.module.register_forward_hook(self.hook)\n",
        "\n",
        "    def reset(self):\n",
        "        self.batch_counts.zero_()\n",
        "\n",
        "    def hook(self, module, inputs, output):\n",
        "        # inputs[0] is 'indices' passed to xFormerEmbeddingBag.forward\n",
        "        indices = inputs[0]  # shape: [N, bag_size]\n",
        "        with torch.no_grad():\n",
        "            flat = indices.reshape(-1).detach().to(\"cpu\")\n",
        "            counts_cpu = torch.bincount(flat, minlength=self.mem_size)\n",
        "            counts = counts_cpu.to(self.batch_counts.device)\n",
        "            self.batch_counts += counts\n",
        "\n",
        "    def close(self):\n",
        "        self.handle.remove()\n",
        "\n",
        "# Create one logger per memory layer\n",
        "mem_loggers = {}\n",
        "mem_sizes = {}\n",
        "\n",
        "for idx in layers_to_replace:  # e.g. [6, 12, 18]\n",
        "    mem_layer = model.model.layers[idx].mlp  # this is HashingMemory\n",
        "    mem_loggers[idx] = MemoryAccessLogger(mem_layer.values)\n",
        "    mem_sizes[idx] = mem_layer.values.weight.shape[0]\n",
        "\n",
        "print(\"\\n‚úÖ MemoryAccessLogger attached for layers:\", layers_to_replace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B3RHCohCytyC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3RHCohCytyC",
        "outputId": "1de14844-c612-40d2-eb2a-2f51ebd3942a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing background DF stats for sparse memory finetuning...\n",
            "Layer 6: background batches = 200\n",
            "Layer 12: background batches = 200\n",
            "Layer 18: background batches = 200\n",
            "\n",
            "Trainable parameters after adding sparse hooks:\n",
            "   model.layers.6.mlp.keys\n",
            "   model.layers.6.mlp.values.weight\n",
            "   model.layers.6.mlp.value_proj.weight\n",
            "   model.layers.6.mlp.value_proj.bias\n",
            "   model.layers.6.mlp.swilu_projection.weight\n",
            "   model.layers.6.mlp.swilu_projection.bias\n",
            "   model.layers.6.mlp.query_proj.query_mlps.0.weight\n",
            "   model.layers.6.mlp.query_proj.query_mlps.0.bias\n",
            "   model.layers.12.mlp.keys\n",
            "   model.layers.12.mlp.values.weight\n",
            "   model.layers.12.mlp.value_proj.weight\n",
            "   model.layers.12.mlp.value_proj.bias\n",
            "   model.layers.12.mlp.swilu_projection.weight\n",
            "   model.layers.12.mlp.swilu_projection.bias\n",
            "   model.layers.12.mlp.query_proj.query_mlps.0.weight\n",
            "   model.layers.12.mlp.query_proj.query_mlps.0.bias\n",
            "   model.layers.18.mlp.keys\n",
            "   model.layers.18.mlp.values.weight\n",
            "   model.layers.18.mlp.value_proj.weight\n",
            "   model.layers.18.mlp.value_proj.bias\n",
            "   model.layers.18.mlp.swilu_projection.weight\n",
            "   model.layers.18.mlp.swilu_projection.bias\n",
            "   model.layers.18.mlp.query_proj.query_mlps.0.weight\n",
            "   model.layers.18.mlp.query_proj.query_mlps.0.bias\n",
            "\n",
            "‚úÖ SparseMemoryTrainer is set up. Your existing `trainer.train()` cell will now do sparse memory finetuning.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import Trainer\n",
        "\n",
        "# 1) Compute background document frequency (DF) over memory slots\n",
        "background_dataset = tokenized\n",
        "\n",
        "bg_loader = DataLoader(\n",
        "    background_dataset,\n",
        "    batch_size=1, # small batch to avoid OOM\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "bg_df = {\n",
        "    idx: torch.zeros(mem_sizes[idx], dtype=torch.int64, device=device)\n",
        "    for idx in layers_to_replace\n",
        "}\n",
        "bg_num_batches = {idx: 0 for idx in layers_to_replace}\n",
        "\n",
        "model.eval()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Computing background DF stats for sparse memory finetuning...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for step, batch in enumerate(bg_loader):\n",
        "        if step >= BACKGROUND_MAX_BATCHES:\n",
        "            break\n",
        "\n",
        "        # Reset per-batch counts for each memory layer\n",
        "        for logger in mem_loggers.values():\n",
        "            logger.reset()\n",
        "\n",
        "        # Move to device and DROP labels to avoid computing loss\n",
        "        batch = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
        "\n",
        "        _ = model(**batch)  # forward only\n",
        "\n",
        "        # For each layer, mark which slots were touched in THIS batch\n",
        "        for idx, logger in mem_loggers.items():\n",
        "            used = (logger.batch_counts > 0).to(bg_df[idx].dtype)\n",
        "            bg_df[idx] += used\n",
        "            bg_num_batches[idx] += 1\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "for idx in layers_to_replace:\n",
        "    print(f\"Layer {idx}: background batches = {bg_num_batches[idx]}\")\n",
        "\n",
        "model.train()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 2) Per-slot trainability masks and gradient hooks:\n",
        "#    only the top-t (TF-IDF) slots will get gradient each batch.\n",
        "slot_train_masks = {\n",
        "    idx: torch.zeros(mem_sizes[idx], dtype=torch.bool, device=device)\n",
        "    for idx in layers_to_replace\n",
        "}\n",
        "\n",
        "def make_grad_hook(layer_idx):\n",
        "    def hook(grad):\n",
        "        \"\"\"\n",
        "        grad: [num_slots, value_dim] for this layer's values.weight\n",
        "        We zero-out rows whose mask is False, so only top-t slots get gradient.\n",
        "        \"\"\"\n",
        "        mask = slot_train_masks[layer_idx].to(grad.device)  # [num_slots]\n",
        "        return grad * mask.unsqueeze(-1)\n",
        "    return hook\n",
        "\n",
        "# Attach hooks to each memory values table\n",
        "for idx in layers_to_replace:\n",
        "    values_param = model.model.layers[idx].mlp.values.weight\n",
        "    values_param.register_hook(make_grad_hook(idx))\n",
        "\n",
        "print(\"\\nTrainable parameters after adding sparse hooks:\")\n",
        "for n, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(\"  \", n)\n",
        "\n",
        "# 3) SparseMemoryTrainer: override compute_loss with the new signature\n",
        "#    NOTE: num_items_in_batch is added to match HF Trainer API.\n",
        "\n",
        "class SparseMemoryTrainer(Trainer):\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        model,\n",
        "        inputs,\n",
        "        return_outputs: bool = False,\n",
        "        num_items_in_batch: int = None,\n",
        "    ):\n",
        "        # Reset per-batch counts for loggers\n",
        "        for logger in mem_loggers.values():\n",
        "            logger.reset()\n",
        "\n",
        "        # Standard forward pass (with labels, so we get loss)\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Compute TF-IDF score for each memory slot, per layer, and choose top-t\n",
        "        with torch.no_grad():\n",
        "            for idx in layers_to_replace:\n",
        "                logger = mem_loggers[idx]\n",
        "                counts = logger.batch_counts # c(i) on this batch\n",
        "                total = counts.sum()\n",
        "\n",
        "                if total == 0:\n",
        "                    slot_train_masks[idx].fill_(False)\n",
        "                    continue\n",
        "\n",
        "                # Term frequency: c(i) / sum_j c(j)\n",
        "                tf = counts.float() / total\n",
        "\n",
        "                # Document frequency from background: df(i) = #background batches where slot i was used\n",
        "                df = bg_df[idx].float()\n",
        "                N = float(bg_num_batches[idx])  # |B|\n",
        "\n",
        "                # Paper‚Äôs TF-IDF:\n",
        "                # c(i)/sum_j c(j) * log( (|B| + 1) / (df(i) + 1) )\n",
        "                idf = torch.log((N + 1.0) / (df + 1.0))\n",
        "                tfidf = tf * idf.to(tf.device)\n",
        "\n",
        "                # Don't pick slots that weren't used in this batch\n",
        "                tfidf = tfidf.masked_fill(counts == 0, float(\"-inf\"))\n",
        "\n",
        "                num_active = int((counts > 0).sum().item())\n",
        "                k = min(SPARSE_TOP_T, num_active) if num_active > 0 else 0\n",
        "\n",
        "                if k == 0:\n",
        "                    slot_train_masks[idx].fill_(False)\n",
        "                    continue\n",
        "\n",
        "                # Top-t indices by TF-IDF\n",
        "                _, topk_idx = torch.topk(tfidf, k=k)\n",
        "                mask = torch.zeros_like(slot_train_masks[idx])\n",
        "                mask[topk_idx] = True\n",
        "                slot_train_masks[idx] = mask\n",
        "\n",
        "        if return_outputs:\n",
        "            return loss, outputs\n",
        "        return loss\n",
        "\n",
        "# Initialize Evaluator\n",
        "evaluator = ModelEvaluator(model, tokenizer, device=device)\n",
        "\n",
        "# Initialize callback\n",
        "memory_monitor = MemoryLayerMonitorAndCheckpoint(\n",
        "    model=model,\n",
        "    layers_to_check=layers_to_replace,\n",
        "    save_every=500,\n",
        "    keep_last=2,\n",
        "    monitor_every=50,\n",
        "    evaluator=evaluator,\n",
        "    eval_every=100,     # Run evaluation every 100 steps\n",
        "    eval_samples=20     # Small sample size for speed during training\n",
        ")\n",
        "\n",
        "# 4) Recreate 'trainer' using SparseMemoryTrainer (overwrites the earlier Trainer)\n",
        "trainer = SparseMemoryTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    eval_dataset=tokenized.select(range(1000)), # same eval subset as before\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[memory_monitor], # keep your monitor/checkpoints\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ SparseMemoryTrainer is set up. trainer.train() cell will now do sparse memory finetuning.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "6345a535",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "6345a535",
        "outputId": "468a6cdd-5d86-4d3a-e3a1-64f30438c6e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Starting training...\n",
            "Total steps: 1437\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   4/1440 00:03 < 45:01, 0.53 it/s, Epoch 0.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 536.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 152.12 MiB is free. Process 8929 has 14.59 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 526.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-279436352.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n‚úÖ Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-180814164.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Standard forward pass (with labels, so we get loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     decorate_autocast.__script_unsupported = (  # type: ignore[attr-defined]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         return CausalLMOutputWithPast(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mForCausalLMLoss\u001b[0;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Enable model parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mshift_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mfixed_cross_entropy\u001b[0;34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> torch.Tensor:\n\u001b[1;32m     35\u001b[0m     \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sum\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# just in case users pass an int for num_items_in_batch, which could be the case for custom trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3459\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 536.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 152.12 MiB is free. Process 8929 has 14.59 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 526.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "print(\"\\nüöÄ Starting training...\")\n",
        "print(f\"Total steps: {len(tokenized) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fbbab63",
      "metadata": {
        "id": "6fbbab63"
      },
      "outputs": [],
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "hidden_dim = 896\n",
        "layers_to_replace = [6, 12, 18]\n",
        "\n",
        "# Reload model for testing\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    dtype=torch.float16,\n",
        ").to(device)\n",
        "\n",
        "# Add memory layers\n",
        "for idx in layers_to_replace:\n",
        "    # Initialize and cast to correct device/dtype\n",
        "    mem_layer = HashingMemory(\n",
        "        input_dim=hidden_dim, output_dim=hidden_dim, mem_n_keys=128, mem_heads=4,\n",
        "        mem_knn=16, mem_k_dim=256, mem_v_dim=-1, swilu_projection=True,\n",
        "        value_fixed_lr=0.001, mem_share_values=False\n",
        "    )\n",
        "    # Important: Cast to model's dtype (float16) to avoid \"Half and Float\" errors\n",
        "    model.model.layers[idx].mlp = mem_layer.to(device, dtype=model.dtype)\n",
        "\n",
        "# Load weights\n",
        "try:\n",
        "    state_dict = load_file(\"./qwen_memory_final/model.safetensors\")\n",
        "except:\n",
        "    state_dict = torch.load(\"./qwen_memory_final/pytorch_model.bin\",\n",
        "                           weights_only=False)\n",
        "\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "print(\"\\n‚úÖ Model loaded successfully!\")\n",
        "\n",
        "# Test generation\n",
        "def test_model(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Try some prompts\n",
        "test_prompts = [\n",
        "    \"Explain quantum computing in simple terms:\",\n",
        "    \"Write a Python function to sort a list:\",\n",
        "    \"What are the health benefits of exercise?\",\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    response = test_model(prompt)\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9e1807",
      "metadata": {
        "id": "3f9e1807"
      },
      "outputs": [],
      "source": [
        "# Load original Qwen model for comparison\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "base_model.to(device)\n",
        "\n",
        "def compare_models(prompt):\n",
        "    # Your fine-tuned model\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Fine-tuned\n",
        "        ft_outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "        ft_response = tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Base\n",
        "        base_outputs = base_model.generate(**inputs, max_new_tokens=100)\n",
        "        base_response = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nüî∑ BASE MODEL:\")\n",
        "    print(base_response)\n",
        "    print(f\"\\nüî∂ FINE-TUNED (with memory layers):\")\n",
        "    print(ft_response)\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Test\n",
        "compare_models(\"Explain machine learning:\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
