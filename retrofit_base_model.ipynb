{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d9018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagoyal/research/finetuningmemorylayers/.venv/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Simplified imports for single GPU\n",
    "from logging import getLogger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer\n",
    "from memory_layers import HashingMemory, MemoryLayerMonitorAndCheckpoint, load_and_process_dataset, load_wikitext_dataset, ModelEvaluator\n",
    "\n",
    "logger = getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d040a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Load Qwen0.5 Instruct\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", dtype=torch.float16).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", dtype=torch.float16)\n",
    "\n",
    "# Qwen0.5 specs: 896 hidden_dim, 24 layers\n",
    "hidden_dim = 896\n",
    "layers_to_replace = [6, 12, 18]  # Which FFN layers to replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e0f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 24406\n",
      "Tokenized dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 20000\n",
      "})\n",
      "Training set size: 18000\n",
      "Evaluation set size: 2000\n"
     ]
    }
   ],
   "source": [
    "# Load and process dataset\n",
    "# Using WikiText for distillation as requested\n",
    "full_dataset = load_wikitext_dataset(tokenizer, sample_size=20000)\n",
    "\n",
    "# Split into train and eval for IterableDataset\n",
    "# Since load_wikitext_dataset returns an IterableDataset (streaming=True)\n",
    "eval_dataset = full_dataset.take(1000)\n",
    "train_dataset = full_dataset.skip(1000)\n",
    "\n",
    "print(\"Datasets loaded (Iterable).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f192f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# print(\"ðŸ“Š Evaluating Base Instruct Model Baseline...\")\n",
    "# # Create a temporary trainer just for evaluation\n",
    "# base_trainer = Trainer(\n",
    "#     model=model,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "#     # report_to=\"tensorboard\",\n",
    "# )\n",
    "# base_metrics = base_trainer.evaluate()\n",
    "# print(f\"Base Instruct Model Eval Loss: {base_metrics['eval_loss']:.4f}\")\n",
    "# print(f\"Base Instruct Model Perplexity: {torch.exp(torch.tensor(base_metrics['eval_loss'])):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909914e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced layer 6 FFN with memory layer\n",
      "Replaced layer 12 FFN with memory layer\n",
      "Replaced layer 18 FFN with memory layer\n",
      "\n",
      "Trainable: 506,820,736 / 506,820,736 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "LAYERS_TO_REPLACE = [6, 12, 18]\n",
    "HIDDEN_DIM = 896\n",
    "BATCH_SIZE = 4\n",
    "STEPS = 500 # Reduced for demo/speed, increase for real training\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Helper to capture input/output of the specific FFN layer\n",
    "class IOCollector:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.target = None\n",
    "\n",
    "    def hook(self, module, input, output):\n",
    "        self.input = input[0].detach() # The hidden state entering FFN\n",
    "        self.target = output.detach()  # The output of the FFN\n",
    "\n",
    "# Distillation Loop Function\n",
    "def distill_layer(layer_idx, model, tokenizer, dataset, device):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Starting Distillation for Layer {layer_idx}\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # 1. Initialize Student Memory Layer\n",
    "    student_memory = HashingMemory(\n",
    "        input_dim=HIDDEN_DIM,\n",
    "        output_dim=HIDDEN_DIM,\n",
    "        mem_n_keys=128,\n",
    "        mem_heads=4,\n",
    "        mem_knn=16,\n",
    "        mem_k_dim=256,\n",
    "        mem_v_dim=-1,\n",
    "        swilu_projection=True,\n",
    "        value_fixed_lr=0.001,\n",
    "        mem_share_values=False,\n",
    "    ).to(device)\n",
    "    # Important: Cast to model's dtype\n",
    "    student_memory = student_memory.to(dtype=model.dtype)\n",
    "    student_memory.train()\n",
    "\n",
    "    # 2. Setup Optimizer\n",
    "    optimizer = torch.optim.AdamW(student_memory.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # 3. Attach hook to the target layer's MLP\n",
    "    target_layer = model.model.layers[layer_idx].mlp\n",
    "    collector = IOCollector()\n",
    "    hook_handle = target_layer.register_forward_hook(collector.hook)\n",
    "\n",
    "    # 4. Data Loader\n",
    "    # Create a fresh dataloader for each layer to ensure we iterate through data\n",
    "    # Since dataset is Iterable, we just iterate it directly\n",
    "    \n",
    "    step = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Iterate through dataset\n",
    "    for batch in dataset:\n",
    "        if step >= STEPS: break\n",
    "        \n",
    "        # A. Run Teacher (Forward pass up to the layer)\n",
    "        # We only need to run the model; the hook captures the data automatically.\n",
    "        inputs = torch.tensor(batch['input_ids']).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # We can stop early to save compute if we knew how, but for now run full forward\n",
    "            # Ideally we'd use a custom forward that stops at layer_idx\n",
    "            model(inputs)\n",
    "        \n",
    "        # Retrieve captured data\n",
    "        x_input = collector.input  # [Batch, Seq, Dim]\n",
    "        y_target = collector.target # [Batch, Seq, Dim]\n",
    "        \n",
    "        # B. Run Student (Memory Layer)\n",
    "        # We pass the EXACT same input the FFN saw\n",
    "        # HashingMemory returns just the output tensor in forward()\n",
    "        y_pred = student_memory(x_input)\n",
    "        \n",
    "        # C. Calculate Loss\n",
    "        # Main Task: Reconstruction (MSE)\n",
    "        mse_loss = F.mse_loss(y_pred, y_target)\n",
    "        \n",
    "        # Aux Task: Load Balancing (if available in your implementation)\n",
    "        # Assuming standard HashingMemory implementation might not return aux loss in forward\n",
    "        # If it does, adjust here. For now, just MSE.\n",
    "        \n",
    "        loss = mse_loss\n",
    "        \n",
    "        # D. Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: MSE={loss.item():.6f}\")\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "    # Cleanup\n",
    "    hook_handle.remove()\n",
    "    avg_loss = total_loss / step\n",
    "    print(f\"Layer {layer_idx} Distillation Complete. Avg MSE: {avg_loss:.6f}\")\n",
    "    \n",
    "    return student_memory\n",
    "\n",
    "# Run distillation for each layer\n",
    "distilled_layers = {}\n",
    "\n",
    "# We need a fresh iterator for each layer or reset it\n",
    "# Since it's an iterable dataset, we can't easily reset. \n",
    "# We'll re-instantiate the dataset iterator or just continue if it's large enough.\n",
    "# Given sample_size=20000 and STEPS=500, we have plenty of data.\n",
    "# We can just continue iterating the same dataset object if we are careful.\n",
    "# Or better, reload it to be safe and consistent.\n",
    "\n",
    "for idx in LAYERS_TO_REPLACE:\n",
    "    # Reload dataset to ensure fresh stream for each layer\n",
    "    # (Optional but good for consistency)\n",
    "    current_dataset = load_wikitext_dataset(tokenizer, sample_size=5000) \n",
    "    \n",
    "    distilled_memory = distill_layer(idx, model, tokenizer, current_dataset, device)\n",
    "    distilled_layers[idx] = distilled_memory\n",
    "    \n",
    "    # Save individual layer\n",
    "    torch.save(distilled_memory.state_dict(), f\"distilled_memory_layer_{idx}.pt\")\n",
    "\n",
    "print(\"\\nAll layers distilled and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5839d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration: Swap in the distilled layers\n",
    "print(\"\\nIntegrating distilled layers into the model...\")\n",
    "\n",
    "for layer_idx, memory_layer in distilled_layers.items():\n",
    "    # Ensure it's on the right device/dtype\n",
    "    memory_layer = memory_layer.to(device, dtype=model.dtype)\n",
    "    \n",
    "    # Replace\n",
    "    model.model.layers[layer_idx].mlp = memory_layer\n",
    "    print(f\"Replaced layer {layer_idx} with distilled memory layer\")\n",
    "\n",
    "# Save the final retrofitted model\n",
    "output_path = \"./qwen_memory_retrofitted\"\n",
    "model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "print(f\"Saved retrofitted model to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c10afd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from safetensors\n",
      "\n",
      "âœ… Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "# Reload model for verification\n",
    "print(\"\\nVerifying retrofitted model...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "hidden_dim = 896\n",
    "layers_to_replace = [6, 12, 18]\n",
    "\n",
    "# Load base model again\n",
    "model_verify = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "# Add memory layers structure\n",
    "for idx in layers_to_replace:\n",
    "    mem_layer = HashingMemory(\n",
    "        input_dim=hidden_dim, output_dim=hidden_dim, mem_n_keys=128, mem_heads=4,\n",
    "        mem_knn=16, mem_k_dim=256, mem_v_dim=-1, swilu_projection=True,\n",
    "        value_fixed_lr=0.001, mem_share_values=False\n",
    "    )\n",
    "    model_verify.model.layers[idx].mlp = mem_layer.to(device, dtype=model_verify.dtype)\n",
    "\n",
    "# Load the saved weights\n",
    "try:\n",
    "    state_dict = load_file(\"./qwen_memory_retrofitted/model.safetensors\")\n",
    "    print(\"Loaded from safetensors\")\n",
    "except:\n",
    "    state_dict = torch.load(\"./qwen_memory_retrofitted/pytorch_model.bin\", weights_only=False)\n",
    "    print(\"Loaded from pytorch_model.bin\")\n",
    "\n",
    "model_verify.load_state_dict(state_dict, strict=False)\n",
    "print(\"âœ… Retrofitted model loaded successfully!\")\n",
    "\n",
    "# Test generation\n",
    "def test_model(model, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nTest Generation:\")\n",
    "print(test_model(model_verify, \"Explain quantum computing in one sentence:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a40c08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Evaluating Base Model With Init Memory Values...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model With Init Memory Values Eval Loss: 2.0576\n",
      "Base Model With Init Memory Values Perplexity: 7.8269\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Evaluating Base Model With Init Memory Values...\")\n",
    "# Create a temporary trainer just for evaluation\n",
    "base_trainer = Trainer(\n",
    "    model=model,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "base_metrics = base_trainer.evaluate()\n",
    "print(f\"Base Model With Init Memory Values Eval Loss: {base_metrics['eval_loss']:.4f}\")\n",
    "print(f\"Base Model With Init Memory Values Perplexity: {torch.exp(torch.tensor(base_metrics['eval_loss'])):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
